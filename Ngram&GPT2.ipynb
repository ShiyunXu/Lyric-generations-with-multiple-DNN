{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jiaruo_Lyrics_Generation(Ngram & GPT2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEZkUBOsvcCV",
        "colab_type": "text"
      },
      "source": [
        "## 0. Installments and Downloads\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nw8fmzRdKdx",
        "colab_type": "code",
        "outputId": "614a81a3-3bc0-47dd-c70b-e58f576bd960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "# download the data from google drive and unzip \n",
        "!gdown https://drive.google.com/uc?id=1OYcrdW8VdQrNQ-P0d8UotHx5QmY7TQaY\n",
        "!unzip 'previous material.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OYcrdW8VdQrNQ-P0d8UotHx5QmY7TQaY\n",
            "To: /content/previous material.zip\n",
            "73.1MB [00:00, 102MB/s] \n",
            "Archive:  previous material.zip\n",
            "   creating: data/\n",
            " extracting: Taylor_lyrics.ipynb     \n",
            " extracting: data/songdata.csv       \n",
            " extracting: data/tweet_sample.csv   \n",
            " extracting: data/Taylor_lyrics.ipynb  \n",
            " extracting: data/taylor_swift_lyrics.csv  \n",
            " extracting: data/processed_pop_sample.csv  \n",
            " extracting: data/taylor_swift_lyrics_sample.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7hS8yJ8v2AY",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data Loading and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWwTodDpZfs_",
        "colab_type": "code",
        "outputId": "ef6a3bed-f09c-4b0c-833a-f8c2dc96ac6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import codecs\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "from nltk.util import ngrams\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Wp4d20IhuE",
        "colab_type": "code",
        "outputId": "190a8542-8eba-4187-dc2d-52a19e6f7da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# import the dataset of taylor swift lyrics\n",
        "dataset = pd.read_csv('data/taylor_swift_lyrics.csv', encoding='latin1')\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>album</th>\n",
              "      <th>track_title</th>\n",
              "      <th>track_n</th>\n",
              "      <th>lyric</th>\n",
              "      <th>line</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>He said the way my blue eyes shined</td>\n",
              "      <td>1</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>Put those Georgia stars to shame that night</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>I said, \"That's a lie\"</td>\n",
              "      <td>3</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>Just a boy in a Chevy truck</td>\n",
              "      <td>4</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>That had a tendency of gettin' stuck</td>\n",
              "      <td>5</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         artist         album  ... line  year\n",
              "0  Taylor Swift  Taylor Swift  ...    1  2006\n",
              "1  Taylor Swift  Taylor Swift  ...    2  2006\n",
              "2  Taylor Swift  Taylor Swift  ...    3  2006\n",
              "3  Taylor Swift  Taylor Swift  ...    4  2006\n",
              "4  Taylor Swift  Taylor Swift  ...    5  2006\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RClY6hWJOu4",
        "colab_type": "code",
        "outputId": "53ff3681-c5d5-46e5-8e70-d99af40451f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "# sort out all songs & merge lyrics from the same song\n",
        "merged_dataset = dataset.groupby(by=['track_title'])['lyric'].agg(lambda x: '. '.join(x))\n",
        "merged_dataset = pd.DataFrame(merged_dataset).reset_index()\n",
        "merged_dataset.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>track_title</th>\n",
              "      <th>lyric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>...Ready for It?</td>\n",
              "      <td>Knew he was a killer first time that I saw him...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22</td>\n",
              "      <td>It feels like a perfect night to dress up like...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Perfectly Good Heart</td>\n",
              "      <td>Why would you wanna break a perfectly good hea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Place In This World</td>\n",
              "      <td>I don't know what I want, so don't ask me. Cau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>All Too Well</td>\n",
              "      <td>I walked through the door with you, the air wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>All You Had to Do Was Stay</td>\n",
              "      <td>People like you always want back. The love the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Back To December</td>\n",
              "      <td>I'm so glad you made time to see me. How's lif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Bad Blood</td>\n",
              "      <td>'Cause baby, now we've got bad blood. You know...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Begin Again</td>\n",
              "      <td>Took a deep breath in the mirror. He didn't li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Better Than Revenge</td>\n",
              "      <td>Now go stand in the corner and think about wha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  track_title                                              lyric\n",
              "0            ...Ready for It?  Knew he was a killer first time that I saw him...\n",
              "1                          22  It feels like a perfect night to dress up like...\n",
              "2      A Perfectly Good Heart  Why would you wanna break a perfectly good hea...\n",
              "3       A Place In This World  I don't know what I want, so don't ask me. Cau...\n",
              "4                All Too Well  I walked through the door with you, the air wa...\n",
              "5  All You Had to Do Was Stay  People like you always want back. The love the...\n",
              "6            Back To December  I'm so glad you made time to see me. How's lif...\n",
              "7                   Bad Blood  'Cause baby, now we've got bad blood. You know...\n",
              "8                 Begin Again  Took a deep breath in the mirror. He didn't li...\n",
              "9         Better Than Revenge  Now go stand in the corner and think about wha..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELQzRqxfXl-7",
        "colab_type": "text"
      },
      "source": [
        "## N-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LfAbHMfZf1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [sent+'.' for sent in dataset['lyric'].values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xey_PZaFvVdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dealing with format, such as punctuation\n",
        "def deal_with_punct(words):\n",
        "  punctuations = {'.': '. ', ',': ', ',';': '; ',':': ': ','...': '... ',\n",
        "           '?': '? ', '!': '! ','(': ' (', ')': ') ', '[': ' [',\n",
        "           ']':  '] ', '``':' \"', \"''\": '\" '}\n",
        "  ans = ' '\n",
        "  for word in words:\n",
        "    if word in punctuations:\n",
        "      punct = punctuations[word]\n",
        "      if ans[-1]==' ':\n",
        "        ans = ans[:-1] + punct\n",
        "      else:\n",
        "        ans += punct\n",
        "    else:\n",
        "      ans += word + ' '\n",
        "  return ans[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NquKXAJxSRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# next word prediction based on frequencies\n",
        "def nextword(ngram, cfdist=None):\n",
        "  xk = np.arange(cfdist[ngram].B())\n",
        "  pk = []\n",
        "  candidate = []\n",
        "  for next_word in cfdist[ngram]:\n",
        "    candidate.append(next_word)\n",
        "    pk.append(cfdist[ngram].freq(next_word))\n",
        "\n",
        "  custm = stats.rv_discrete(values=(xk,pk))\n",
        "  return candidate[custm.rvs()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLM8iw0SgUsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating lyrics\n",
        "def generate_lyrics(sentences, n, num_sents):\n",
        "  tok_sents = [[None for _ in range(n-1)] + word_tokenize(sent) for sent in sentences]\n",
        "  words = []\n",
        "  for tok_sent in tok_sents:\n",
        "    words += tok_sent\n",
        "  words += [None]\n",
        "  ngrams_ = ngrams(words, n)\n",
        "  ngrams_cfd = ConditionalFreqDist((ngram[:-1], ngram[-1]) for ngram in ngrams_)\n",
        "\n",
        "  text_input = tuple(None for _ in range(n-1))\n",
        "  text_gen = []\n",
        "  num_sents_gen = 0\n",
        "  while True:\n",
        "    next_word = nextword(text_input, ngrams_cfd)\n",
        "    if not next_word:\n",
        "      num_sents_gen += 1\n",
        "      if num_sents and num_sents==num_sents_gen:\n",
        "        break\n",
        "      text_input = tuple(None for _ in range(n-1))\n",
        "      continue\n",
        "\n",
        "    text_input += (next_word,) \n",
        "    text_input = text_input[1:]\n",
        "    text_gen.append(next_word)\n",
        "  return deal_with_punct(text_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YemDF9nEBRWZ",
        "colab_type": "code",
        "outputId": "1df649b7-7263-4b6c-b367-7b0216c381cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "size = 15\n",
        "outputs = []\n",
        "for i in range(size):\n",
        "  outputs.append(generate_lyrics(sentences, 4, num_sents=1))\n",
        "\n",
        "outputs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" We found Wonderland, you and I swear I do n't wan na touch you, I run and run.\",\n",
              " \" 'Cause it 's late and your mama do n't know how my friends could be so mean.\",\n",
              " \" And I would 've been so happy.\",\n",
              " ' Had me in the eye and told me you loved me.',\n",
              " ' I just want to know you better now.',\n",
              " ' Dating the boy on the football team.',\n",
              " ' Cause we never go out of style.',\n",
              " \" 'Cause all I know is.\",\n",
              " ' Are you ready for it?.',\n",
              " ' On all my wasted time.',\n",
              " ' And now you say\\x97.',\n",
              " ' And everybody knows that.',\n",
              " \" My baby 's fit like a daydream.\",\n",
              " ' I am not the kind of girl.',\n",
              " \" And I do n't know what to say.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzIbN2kQB0_D",
        "colab_type": "text"
      },
      "source": [
        "## Finetune-GPT2 Model\n",
        "\n",
        "Here we basically used pre-trained GPT-2 model to transfer learning on our task. The model is simply for comparing results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od-gJoKxCVr1",
        "colab_type": "code",
        "outputId": "896b3894-a035-4b27-dd13-77fd59b6f5a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=188gwThRiEuAXNIkbaKLjOktJwZvEnIt5\n",
        "!unzip new_finetune.zip\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1EPzPWHAsuAfoRpsA3d4qA-ogJyLf-7G0  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=188gwThRiEuAXNIkbaKLjOktJwZvEnIt5\n",
            "To: /content/new_finetune.zip\n",
            "463MB [00:03, 118MB/s]\n",
            "Archive:  new_finetune.zip\n",
            "   creating: new_finetune/\n",
            "  inflating: new_finetune/tokenizer_config.json  \n",
            "  inflating: new_finetune/special_tokens_map.json  \n",
            "  inflating: new_finetune/config.json  \n",
            "  inflating: new_finetune/merges.txt  \n",
            "  inflating: __MACOSX/new_finetune/._merges.txt  \n",
            "  inflating: new_finetune/training_args.bin  \n",
            "  inflating: new_finetune/pytorch_model.bin  \n",
            "  inflating: __MACOSX/new_finetune/._pytorch_model.bin  \n",
            "  inflating: new_finetune/vocab.json  \n",
            "  inflating: new_finetune/eval_results.txt  \n",
            "  inflating: __MACOSX/new_finetune/._eval_results.txt  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EPzPWHAsuAfoRpsA3d4qA-ogJyLf-7G0\n",
            "To: /content/run_generation.py\n",
            "100% 10.1k/10.1k [00:00<00:00, 25.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW2rQYlIC2GF",
        "colab_type": "code",
        "outputId": "5f39479b-f980-478b-e04b-ea575f0bc432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        }
      },
      "source": [
        "!python -m pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 19.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 22.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.16.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5dab5a403fb1ec7eae833183f4921d123e0450a32fdf0c24fa5e069d95eb043c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5pPzw4_DesH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change input sentence here to generate different output \n",
        "sentence = \"She's not a saint and she's not what you think She's an actress, whoa She's better known for the things that she does On the mattress, whoa\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lse2pNVDnV9",
        "colab_type": "text"
      },
      "source": [
        "change the value of \"length\" will change the length of output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfc9esf3Dimk",
        "colab_type": "code",
        "outputId": "9304d762-25da-4dd6-d7c4-ab4869411e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 run_generation.py \\\n",
        "    --model_type=gpt2 \\\n",
        "    --length=25 \\\n",
        "    --model_name_or_path='./new_finetune' \\\n",
        "    --prompt=\"$sentence\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-07 02:55:46.232416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "05/07/2020 02:55:48 - INFO - transformers.tokenization_utils -   Model name './new_finetune' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming './new_finetune' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/07/2020 02:55:48 - INFO - transformers.tokenization_utils -   Didn't find file ./new_finetune/added_tokens.json. We won't load it.\n",
            "05/07/2020 02:55:48 - INFO - transformers.tokenization_utils -   loading file ./new_finetune/vocab.json\n",
            "05/07/2020 02:55:48 - INFO - transformers.tokenization_utils -   loading file ./new_finetune/merges.txt\n",
            "05/07/2020 02:55:48 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/07/2020 02:55:48 - INFO - transformers.tokenization_utils -   loading file ./new_finetune/special_tokens_map.json\n",
            "05/07/2020 02:55:48 - INFO - transformers.tokenization_utils -   loading file ./new_finetune/tokenizer_config.json\n",
            "05/07/2020 02:55:48 - INFO - transformers.configuration_utils -   loading configuration file ./new_finetune/config.json\n",
            "05/07/2020 02:55:48 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "05/07/2020 02:55:48 - INFO - transformers.modeling_utils -   loading weights file ./new_finetune/pytorch_model.bin\n",
            "05/07/2020 02:55:54 - INFO - __main__ -   Namespace(device=device(type='cpu'), k=0, length=25, model_name_or_path='./new_finetune', model_type='gpt2', n_gpu=0, no_cuda=False, num_return_sequences=1, p=0.9, padding_text='', prompt=\"She's not a saint and she's not what you think She's an actress, whoa She's better known for the things that she does On the mattress, whoa\", repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "05/07/2020 02:55:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "She's not a saint and she's not what you think She's an actress, whoa She's better known for the things that she does On the mattress, whoa, this is what happens when a girl has an asthma attack she doesn't talk about who you are, she just says,\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}